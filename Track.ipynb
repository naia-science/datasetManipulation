{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52079fd8-c56a-4362-a44d-550a772a8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6407df1-17d0-4959-8b11-bb55a318eeec",
   "metadata": {},
   "source": [
    "## Create video from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c75298-51b7-4b26-b749-74cd62d20535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set parameters\n",
    "image_path = 'large_image.jpg'  \n",
    "output_folder = 'frames640_2' \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "output_video = 'panned_video_640_2.mp4'\n",
    "frame_size = (640, 640)  \n",
    "total_frames = 300  \n",
    "fps = 30  \n",
    "\n",
    "def curve_function(t):\n",
    "    # t ranges from 0 to 1\n",
    "    x = int(500)  # x position over time\n",
    "    y = int(t * 3000)  # y position over time\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769422d-1eba-42fd-b862-f2803de6033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large image\n",
    "image = cv2.imread(image_path)\n",
    "img_height, img_width = image.shape[:2]\n",
    "\n",
    "# Generate frames\n",
    "for i in range(total_frames):\n",
    "    # Calculate t as a percentage of the total frames\n",
    "    t = i / total_frames\n",
    "    # Get the top-left coordinates for the current frame crop\n",
    "    x, y = curve_function(t)\n",
    "    \n",
    "    # Ensure the cropping window stays within image bounds\n",
    "    x = max(0, min(x, img_width - frame_size[0]))\n",
    "    y = max(0, min(y, img_height - frame_size[1]))\n",
    "\n",
    "    # Crop the frame from the large image\n",
    "    frame = image[y:y + frame_size[1], x:x + frame_size[0]]\n",
    "\n",
    "    # Save the frame\n",
    "    frame_path = os.path.join(output_folder, f'frame_{i:04d}.png')\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    if i%10==0 or i==total_frames-1:\n",
    "        print(f'Saved frame {i+1}/{total_frames}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4722d0-6bdf-46d5-98e6-81956bd90697",
   "metadata": {},
   "source": [
    "### To build the video\n",
    "```bash\n",
    "ffmpeg -framerate 30 -i frames640/frame_%04d.png -c:v libx264 -pix_fmt yuv420p panned_video640.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ab69f-7ce1-4b5c-8d8c-fff85116637c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Count objects on video using solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8cb29-8e12-46e8-bbec-78f904a2d86f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"panned_video.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define region points\n",
    "region_points = [(20, 400), (1080, 404), (1080, 360), (20, 360)]\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output2.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Init Object Counter\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,\n",
    "    region=region_points,\n",
    "    model=\"urbanet11s.pt\",\n",
    "    conf=0.01,\n",
    "    tracker = \"bytetrack.yaml\",\n",
    "    iou = 0.1,\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "    im0 = counter.count(im0)\n",
    "    video_writer.write(im0)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ae4be-cfdf-4fee-8825-02a87eea57e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "model = YOLO(\"urbanet11s.pt\")  # segmentation model\n",
    "names = model.model.names\n",
    "cap = cv2.VideoCapture(\"panned_video.mp4\")\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(\"instance-segmentation_low_conf.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = model.predict(im0, conf=0.03)\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "    if results[0].masks is not None:\n",
    "        clss = results[0].boxes.cls.cpu().tolist()\n",
    "        masks = results[0].masks.xy\n",
    "        for mask, cls in zip(masks, clss):\n",
    "            color = colors(int(cls), True)\n",
    "            txt_color = annotator.get_txt_color(color)\n",
    "            annotator.seg_bbox(mask=mask, mask_color=color, label=names[int(cls)], txt_color=txt_color)\n",
    "\n",
    "    out.write(im0)\n",
    "    #cv2.imshow(\"instance-segmentation\", im0)\n",
    "\n",
    "    #if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    #    break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d93ab2-3689-4327-bce2-bfdc852d610a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "model = YOLO(\"urbanet11s.pt\")  # segmentation model\n",
    "cap = cv2.VideoCapture(\"panned_video.mp4\")\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(\"instance-segmentation-object-tracking-lowconf2.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "    results = model.track(im0, conf=0.03, persist=True)\n",
    "\n",
    "    if results[0].boxes.id is not None and results[0].masks is not None:\n",
    "        masks = results[0].masks.xy\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "        for mask, track_id in zip(masks, track_ids):\n",
    "            color = colors(int(track_id), True)\n",
    "            txt_color = annotator.get_txt_color(color)\n",
    "            annotator.seg_bbox(mask=mask, mask_color=color, label=str(track_id), txt_color=txt_color)\n",
    "\n",
    "    out.write(im0)\n",
    "\n",
    "out.release()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b6328-f213-4827-ab28-241d5b3bbac2",
   "metadata": {},
   "source": [
    "## Using surfnet tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa144583-cc48-45cb-8b2b-8b56ceb92fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os.path as op\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from flask import jsonify, request\n",
    "from werkzeug.utils import secure_filename\n",
    "from typing import List, Tuple\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from plasticorigins.tracking.postprocess_and_count_tracks import (\n",
    "    filter_tracks,\n",
    "    postprocess_for_api,\n",
    ")\n",
    "from plasticorigins.tracking.track_video import track_video\n",
    "from plasticorigins.tracking.trackers import get_tracker\n",
    "from plasticorigins.tracking.utils import (\n",
    "    get_detections_for_video,\n",
    "    read_tracking_results,\n",
    "    write_tracking_results_to_file,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "engine = get_tracker(\"EKF\")\n",
    "\n",
    "def run_video(config) -> json:\n",
    "    # launch the tracking\n",
    "    filtered_results, detections = track(config)\n",
    "\n",
    "    # postprocess\n",
    "    output_json = postprocess_for_api(filtered_results, id_categories)\n",
    "\n",
    "    return output_json, filtered_results, detections\n",
    "\n",
    "\n",
    "def track(args: argparse) -> Tuple[List, int, int]:\n",
    "\n",
    "    def detector(frame):\n",
    "        res = model_yolo(frame[:,:,::-1])\n",
    "        dets = res[0].boxes.xyxy.numpy()\n",
    "        cls = res[0].boxes.cls.numpy()\n",
    "        confs = res[0].boxes.conf.numpy()\n",
    "        # Improve megot scores\n",
    "        confs = np.where(cls == 10, confs * 10, confs)\n",
    "        return dets, confs, cls\n",
    "\n",
    "    reader = IterableFrameReader(\n",
    "        video_filename=args.video_path,\n",
    "        skip_frames=args.skip_frames,\n",
    "        output_shape=args.output_shape,\n",
    "        progress_bar=True,\n",
    "        preload=args.preload_frames,\n",
    "        crop=args.crop,\n",
    "    )\n",
    "\n",
    "    num_frames, fps = (\n",
    "        int(reader.max_num_frames / (args.skip_frames + 1)),\n",
    "        reader.fps,\n",
    "    )\n",
    "\n",
    "    print(\"---Detecting...\")\n",
    "    detections = []\n",
    "    for frame in reader:\n",
    "        detections.append(detector(frame))\n",
    "\n",
    "    print(\"---Tracking...\")\n",
    "    display = None\n",
    "\n",
    "    results = track_video(\n",
    "        reader,\n",
    "        iter(detections),\n",
    "        args,\n",
    "        engine,\n",
    "        transition_variance,\n",
    "        observation_variance,\n",
    "        display,\n",
    "        is_yolo=args.arch == \"yolo\",\n",
    "    )\n",
    "    reader.video.release()\n",
    "    # store unfiltered results\n",
    "    output_filename = Path(args.output_dir) / \"results_unfiltered.txt\"\n",
    "    coord_mapping = reader.get_inv_mapping(args.downsampling_factor)\n",
    "    write_tracking_results_to_file(\n",
    "        results,\n",
    "        coord_mapping,  # Scale the output back to original video size\n",
    "        output_filename=output_filename,\n",
    "    )\n",
    "    print(\"---Filtering...\")\n",
    "\n",
    "    # read from the file\n",
    "    results = read_tracking_results(output_filename)\n",
    "    filtered_results = filter_tracks(results, args.kappa, args.tau)\n",
    "    # store filtered results\n",
    "    output_filename = Path(args.output_dir) / \"results.txt\"\n",
    "    write_tracking_results_to_file(\n",
    "        filtered_results,\n",
    "        lambda x, y: (x, y),  # No scaling, already scaled!\n",
    "        output_filename=output_filename,\n",
    "    )\n",
    "    print(\"---Done.\")\n",
    "\n",
    "    return filtered_results, detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3acff-2be6-400e-9129-f2b79b4a9e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = Path(\"/media/charles/81d75a6a-d733-4070-ad6f-9e7fe046ffab/Programs/datasetManipulation/\")\n",
    "\n",
    "in_video = (base_dir / \"panned_video.mp4\").as_posix()\n",
    "out_folder = (base_dir / \"out5\").as_posix()\n",
    "if not op.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config = DotDict(\n",
    "    {\n",
    "        \"confidence_threshold\": 0.15,\n",
    "        \"downsampling_factor\": 1,\n",
    "        \"noise_covariances_path\": \"/media/charles/81d75a6a-d733-4070-ad6f-9e7fe046ffab/Programs/surfnet/data/tracking_parameters\",\n",
    "        \"file_model_yolo\": \"urbanet11s.pt\",\n",
    "        \"output_shape\": (640, 640),\n",
    "        \"skip_frames\": 3,  # 3\n",
    "        \"arch\": \"yolo\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"detection_batch_size\": 1,\n",
    "        \"display\": 0,\n",
    "        \"kappa\": 5,  # 7\n",
    "        \"tau\": 3,  # 4\n",
    "        \"crop\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "id_categories = {\n",
    "    0: \"autre\",\n",
    "    1: \"autre-papier-carton\",\n",
    "    2: \"autre-plastique-fragments\",\n",
    "    3: \"autre-polystyrene\",\n",
    "    4: \"bouteille-en-plastique\",\n",
    "    5: \"bouteille-en-verre\",\n",
    "    6: \"canette\",\n",
    "    7: \"emballage-alimentaire-papier\",\n",
    "    8: \"emballage-alimentaire-plastique\",\n",
    "    9: \"indefini\",\n",
    "    10: \"megot\",\n",
    "    11: \"sac-ordures-menageres\"\n",
    "}\n",
    "\n",
    "model_yolo = YOLO(\"/media/charles/81d75a6a-d733-4070-ad6f-9e7fe046ffab/Programs/datasetManipulation/urbanet11s.pt\")\n",
    "\n",
    "observation_variance = np.load(\n",
    "    os.path.join(config_track.noise_covariances_path, \"observation_variance.npy\")\n",
    ")\n",
    "transition_variance = np.load(\n",
    "    os.path.join(config_track.noise_covariances_path, \"transition_variance.npy\")\n",
    ")\n",
    "\n",
    "config.video_path = in_video\n",
    "config.output_dir = out_folder\n",
    "\n",
    "out_json, filtered_results, detections = run_video(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b1951-73d1-4f34-b1da-062a27b6a0ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create video\n",
    "from plasticorigins.tracking.utils import generate_video_with_annotations\n",
    "reader = IterableFrameReader(video_filename=config.video_path,\n",
    "                            skip_frames=0,\n",
    "                            progress_bar=True,\n",
    "                            preload=False,\n",
    "                            max_frame=0)\n",
    "\n",
    "def load_trash_icons(folder_path: str) -> Dict:\n",
    "    folder_path = Path(folder_path)\n",
    "    id_path = {\n",
    "        \"autre\": folder_path / \"chaussure.png\",  # 'Fragment',    #'Sheet / tarp / plastic bag / fragment',\n",
    "        \"autre-papier-carton\": folder_path / \"emballage.png\",  # 'Insulating',  #'Insulating material',\n",
    "        \"autre-plastique-fragments\": folder_path / \"fragment.png\",\n",
    "        \"autre-polystyrene\": folder_path / \"mousse.png\",\n",
    "        \"bouteille-en-plastique\": folder_path / \"bouteille.png\",  # 'Bottle',      #'Bottle-shaped',\n",
    "        \"bouteille-en-verre\": folder_path / \"bouteille.png\",  # 'Bottle',      #'Bottle-shaped',\n",
    "        \"canette\": folder_path / \"briquet.png\",  # 'Can',         #'Can-shaped',\n",
    "        \"emballage-alimentaire-papier\": folder_path / \"emballage.png\",  # 'Packaging',   #'Other packaging',\n",
    "        \"emballage-alimentaire-plastique\": folder_path / \"emballage.png\",  # 'Tire',\n",
    "        \"indefini\": folder_path / \"dechet.png\",  # 'Drum',\n",
    "        \"megot\": folder_path / \"megot.png\",  # 'Fishing net', #'Fishing net / cord',\n",
    "        \"sac-ordures-menageres\": folder_path / \"dechet.png\",  # 'Unclear'\n",
    "    }\n",
    "    out_dict = {}\n",
    "\n",
    "    for idx, path in id_path.items():\n",
    "        img = cv2.imread(path.resolve().as_posix(), cv2.IMREAD_UNCHANGED)\n",
    "        resized_img = cv2.resize(img, (100, 60), interpolation=cv2.INTER_AREA)\n",
    "        out_dict[idx] = resized_img\n",
    "\n",
    "    return out_dict\n",
    "\n",
    "labels2icons = load_trash_icons(\"./icons/\")\n",
    "\n",
    "generate_video_with_annotations(reader, out_json, Path(config.output_dir) / \"video.mp4\",\n",
    "                                config.skip_frames, 1,\n",
    "                                logger, gps_data=None, labels2icons=labels2icons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20282a5b-184f-4982-8c59-94f7b7bc8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse resutls\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(filtered_results, columns=['frame', 'idx', 'x', 'y', 'Score', 'Label'])\n",
    "\n",
    "df_filtered = df[df['Label'] == 2]\n",
    "\n",
    "# Group by the 'Second Column' and apply desired aggregation\n",
    "# Here we simply count the occurrences in each group, but you can replace with any other aggregation\n",
    "grouped_df = df_filtered.groupby('idx').agg(\n",
    "    Count=('idx', 'size'),\n",
    "    Average_Score=('Score', 'mean')\n",
    ").reset_index()\n",
    "grouped_df = grouped_df.drop(columns='idx')\n",
    "\n",
    "# Display the result\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7f6a5-003e-4cbd-944d-93b349151bc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81331b90-471b-444c-8d6c-bf7c5925cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_for_frame = next(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05ac8a-3971-455d-8ac3-a684fcd29318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = config_track\n",
    "reader = IterableFrameReader(\n",
    "    video_filename=args.video_path,\n",
    "    skip_frames=args.skip_frames,\n",
    "    output_shape=args.output_shape,\n",
    "    progress_bar=True,\n",
    "    preload=args.preload_frames,\n",
    "    crop=args.crop,\n",
    ")\n",
    "\n",
    "num_frames, fps = (\n",
    "    int(reader.max_num_frames / (args.skip_frames + 1)),\n",
    "    reader.fps,\n",
    ")\n",
    "\n",
    "logger.info(\"---Detecting...\")\n",
    "detections = []\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for frame in reader:\n",
    "        detections.append(detector(frame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87c71d-f544-4c17-a7f7-7a60cc97c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "fram_pic = detections[0][0].orig_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739d09d-6608-4cbd-b9d4-89f3f8bb8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fram_pic[:,:,::-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fceb0e-06d3-48cc-a4f7-446790060f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = postprocess_for_api(filtered_results, id_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919dee95-8cf6-4b94-a610-b9ff2205f794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9711286-d318-4857-a0c9-36e90bff650f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cbe68-fe56-4a2f-84a5-9c3dd886727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(filtered_results, columns=['frame', 'idx', 'x', 'y', 'Score', 'Label'])\n",
    "\n",
    "df_filtered = df[df['Label'] == 2]\n",
    "\n",
    "# Group by the 'Second Column' and apply desired aggregation\n",
    "# Here we simply count the occurrences in each group, but you can replace with any other aggregation\n",
    "grouped_df = df_filtered.groupby('idx').agg(\n",
    "    Count=('idx', 'size'),\n",
    "    Average_Score=('Score', 'mean')\n",
    ").reset_index()\n",
    "grouped_df = grouped_df.drop(columns='idx')\n",
    "\n",
    "# Display the result\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eeac3e-4855-4c45-9483-4727216ea6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plasticorigins.tracking.utils import generate_video_with_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ebd41-c3ee-4750-af60-65cff3f45c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = IterableFrameReader(video_filename=config_track.video_path,\n",
    "                            skip_frames=0,\n",
    "                            progress_bar=True,\n",
    "                            preload=False,\n",
    "                            max_frame=0)\n",
    "\n",
    "generate_video_with_annotations(reader, output_json, Path(args.output_dir) / \"video.mp4\",\n",
    "                                config_track.skip_frames, 1,\n",
    "                                logger, gps_data=None, labels2icons=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d97909-60e8-4ed5-9cbb-e6eade184cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43057e2-9989-4f51-ac48-19886f0a0a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
